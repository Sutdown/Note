#### 协程库面试向

> 3.线程模块
>
> 线程模块，封装了pthread里面的一些常用功能，Thread,Semaphore,Mutex,RWMutex,Spinlock等对象，可以方便开发中对线程日常使用
> 为什么不适用c++11里面的thread
> 本框架是使用C++11开发，不使用thread，是因为thread其实也是基于pthread实现的。并且C++11里面没有提供读写互斥量，RWMutex，Spinlock等，在高并发场景，这些对象是经常需要用到的。所以选择了自己封装pthread
>
> 4.协程模块
>
> 协程：用户态的线程，相当于线程中的线程，更轻量级。后续配置socket hook，可以把复杂的异步调用，封装成同步操作。降低业务逻辑的编写复杂度。
> 目前该协程是基于ucontext_t来实现的，后续将支持采用boost.context里面的fcontext_t的方式实现
>
> 5.协程调度模块
>
> 协程调度器，管理协程的调度，内部实现为一个线程池，支持协程在多线程中切换，也可以指定协程在固定的线程中执行。是一个N-M的协程调度模型，N个线程，M个协程。重复利用每一个线程。
>
> 6.IO协程调度模块
>
> 继承与协程调度器，封装了epoll（Linux），并支持定时器功能（使用epoll实现定时器，精度毫秒级）,支持Socket读写时间的添加，删除，取消功能。支持一次性定时器，循环定时器，条件定时器等功能



> 简要介绍一下协程库这个项目（包括分模块介绍），协程的概念

本项目主要参考开源项目sylar，实现了linux环境下使用C++从零开发的部分服务器框架—协程库。（编写原因：见下，对比进程线程协程）

项目主要实现了协程库的编写，基于ucontext_t实现了协程类，结合epoll和定时器实现了N-M协程调度器，支持IO事件，定时器事件的回调。同时完成了日志功能，配置功能，多线程和锁的封装等工作。

###### 线程封装

通过Thread类封装了线程的创建，执行和销毁，通过Semaphore同步线程的启动，实现了一个基本的线程管理。

该模块中，`t_thread` 变量的主要作用是通过 `thread_local` 保证每个线程独立拥有自己的 `Thread` 对象指针，进而使得线程能够方便地访问和操作自己特有的线程信息。

该代码实现了一个简单的线程管理库，通过 `Thread` 类封装了线程的创建、执行和销毁。它使用 `Semaphore` 来同步线程的启动，确保线程的初始化在执行任务前完成。通过 `pthread_create` 创建线程，通过 `pthread_join` 或 `pthread_detach` 来管理线程的生命周期。`Thread` 类还提供了线程信息（如线程 ID 和名称）的访问方法，方便获取当前线程的相关信息。

```cpp
// 该函数创建一个新的线程，并使其开始执行 start_routine 函数。
int pthread_create(pthread_t *thread, const pthread_attr_t *attr, 
                   void *(*start_routine) (void *), void *arg);

// pthread_join 会阻塞调用线程，直到指定的线程 thread 完成执行并退出。
// 调用线程会获取该线程的返回值，回收该线程的资源。通常用于主线程等待子线程完成并进行一些清理工作。
int pthread_join(pthread_t thread, void **retval);

// 调用 pthread_detach 后，指定的线程将被分离，意味着主线程不再需要等待它完成。
// 线程资源（如栈空间）会在线程退出后自动释放。
int pthread_detach(pthread_t thread);
```



###### 协程实现

基于ucontext_t实现了能够resume和yield的具有独立栈的非对称协程，同时设计三种协程状态，使子协程可以和线程主协程相互切换。

```cpp
// 上下文结构体定义
// 这个结构体是平台相关的，因为不同平台的寄存器不一样
// 下面列出的是所有平台都至少会包含的4个成员
typedef struct ucontext_t {
  // 当前上下文结束后下一个激活的上下文对象的指针，
  // 只在当前上下文是由makecontext创建时有效
  struct ucontext_t *uc_link;
  // 当前上下文的信号屏蔽掩码
  sigset_t uc_sigmask;
  // 当前上下文使用的栈内存空间，只在当前上下文是由makecontext创建时有效
  stack_t uc_stack;
  // 平台相关的上下文具体内容，包含寄存器的值
  mcontext_t uc _mcontext;
...
} ucontext_t;

// 获取当前的上下文
int getcontext(ucontext_t *ucp);

// 恢复ucp指向的上下文，同时跳转到ucp上下文对应的函数中执行
int setcontext(const ucontext_t *ucp);

/* 
修改由getcontext获取到的上下文指针ucp，将其与一个函数func进行绑定，支持指定func运行时的参数，
在调用makecontext之前，必须手动给ucp分配一段内存空间，存储在ucp->uc_stack中，这段内存空间将作为func函数运行时的栈空间，
同时也可以指定ucp->uc_link，表示函数运行结束后恢复uc_link指向的上下文，
如果不赋值uc_link，那func函数结束时必须调用setcontext或swapcontext以重新指定一个有效的上下文，否则程序就跑⻜了
makecontext执行完后，ucp就与函数func绑定了，调用setcontext或swapcontext激活ucp时，func就会被运行
*/
// 修改当前上下文指针ucp，将其与func函数绑定
void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);

/*
swapcontext 函数用于 切换上下文，即保存当前上下文到 oucp 中，并恢复 ucp 指向的上下文。它是 getcontext 和 setcontext 的组合操作。
swapcontext是sylar非对称协程实现的关键，线程主协程和子协程用这个接口进行上下文切换
*/
// 恢复ucp指向的上下文，同时将当前上下文存储到oucp中
int swapcontext(ucontext_t *oucp, const ucontext_t *ucp);
```

设计思路如下：

- 存在三种协程状态READY,RUNNING, TERM就绪运行和结束。
- （非对称）协程之间需要切换，因此设定了当前协程（有参构造）和主协程（私有的无参构造）的线程局部变量，存储上下文信息，便于访问。
- （独立栈，运行函数）协程自身的话，需要绑定运行函数，每个栈有自己固定大小的栈空间，同时reset能够复用栈空间
- 协程有两个操作,resume(恢复协程运行)和yield(让出执行)
  - 具体表现为：切换状态，调用swapcontext
- （构造函数）初始化内存空间，同时用不带参数构造函数初始化主协程（GetThis能够返回当前线程的主协程，并且如果未创建会自主创建）
- （协程入口函数）对线程模块入口函数的封装，能够在协程结束时自动yield
- Other(退出的协程可以重置)



###### 协程调度

实现了了N-M协程调度器。采用线程池和协程任务队列维护了一个调度线程池，同时支持主协程参与调度。

模块实现：

这个调度器需要实现，用户仅仅需要启动调度器，添加协程事件，和停止调度器即可。内部协程间的执行由程序自行实现。

调度器内核维护一个任务队列和一个调度线程池，

开始调度后，线程池从任务队列中取出任务执行，调度线程可以包含caller线程，任务全部执行完后，线程池停止调度，等待新的任务进来，添加新的任务后，通知线程池新任务进来，线程池重新开始运行调度，停止调度时，各调度线程退出，调度器停止工作。

- （支持主协程参与调度，修改fiber）由于需要实现主协程，调度协程，任务协程三者之间的切换，因此在协程类中会添加调度协程，同时记录三者的上下文。

  至于对于主协程的支持，设计了一个bool类型的成员m_runInscheduler，想让调度器调度的协程该值设置为true。resume和yield时，参与协程器调度则是与调度协程上下文切换，不参与则是和主协程切换，其实就是调度协程本身不参与调度和主协程切换。

- （协程调度算法）实现一个list的协程任务队列（调度任务支持函数或者协程），FIFO调度算法。任务的添加删除等会使用自身的互斥锁

- （线程池） std::vector\<std::shared\_ptr<Thread\>> m_threads;。

- （调度器的启动时的运行）初始化线程池，线程会自动绑定run函数，run会不停的仓任务队列取出任务执行，任务队列为空时，代码进入idle协程，idle会直接yield，继续循环，此时大概处于忙等，等待新的协程任务进来。



###### 定时器

基于最小堆实现了定时器功能，设计Timer类和TimerManager类，支持定时事件的添加，删除，更新。

设置了两个类：（两个类互为友元类）

- 定时器类
  - 该类中包含，回调函数，执行周期+是否循环+绝对超时时间，定时器管理器
  - 公共接口：定时器取消，定时器刷新，定时器时间的重置
- 定时器管理类
  - 该类中包含定时器集合，同时考虑定时器时间是否回退
  - 公共接口：添加定时器，找出最近的超时时间，判断是否有定时器，取出超时定时器的所有回调函数



###### 协程IO

在基本的协程调度器基础上结合`epoll`和定时器实现了IO协程调度。利用pipe，定时器和epoll机制重载tickle和idle函数实现最终的协程IO。

背景：在前面的协程调度模块中，任务添加之后就会自动等待调度器的执行。调度器不支持删除调度任务，并且正常退出之前一定会执行完全部的调度任务，而IO协程调度希望在此基础上增加IO时间调度功能，同时加上处理定时器事件。

过程：

（`FdContexts`，待处理的文件描述符）对于IO协程调度来说，每次调度都包含一个**三元组**信息，分别时描述符，事件类型，回调函数，调度器记录全部需要调度的三元组信息，其中描述符和事件类型用于`epoll_wait`，回调函数用于写缓存调度。三元组通过`FdCOntext`存储，`epoll_wait`通过`epoll_event.data.ptr`存储三元组结构体信息

（`pipe`，用于触发事件的管道文件描述符，管道的机制决定epoll_wait的返回）管道充当了一个信号通知机制，通过向管道的写端写入数据，调度器可以通过读取管道来检测到事件的发生，从而恢复挂起的协程。

在继承类`IOManager`中改造协程调度器，使其支持`epoll`，**并重载`tickle`和`idle`，**实现通知调度协程和IO协程调度功能；同时增加定时器管理的基础，实现协程调度器对定时任务的调度

- （重载tickle和idle）`epoll_wait`，tickle的机制均来源于pipe，也就是当发生IO时，yield，将`fd`注册到[0]中，同时将[0]`epoll_ctl`到`epoll`的多路复用中；同时tickle则是直接向[1]中写入字符便可唤醒`epoll_wait`

- （idle的重载）IO协程调度器在`idle`时会`epoll_wait`所有注册的`fd`，如果有`fd`满足条件，

  `epoll_wait`返回，从`epoll_event`中找[0]中同样含有的`fd`的，根据上下文信息，并且执行其中的回调函数。（idle的触发点在于tickle或者注册的IO事件就绪）。

- （注册，删除，取消，取消全部事件）

  - 将新的事件根据描述符注册到`m_fdContext`中，同时更新相应的事件类型，回调函数；同时进行`epoll_ctl`，基本就是ADD的操作类型
  - 从`m_fdContext`中删除并且重置上下文，同时在`epoll`中更新信息，此时应当变为DEL
  - 取消前会进行回调函数；取消所有则是直接将events置为0，同时触发所有的相关事件

  （每个文件描述符下面不止一个事件，注册删除取消都是指对文件描述符的某个事件操作，取消所有则是删除事件，事件的触发重置都是协程的任务）

- （定时器）为idle函数增加了一个出发点，也就是`epoll_wait`应该根据下一个定时器的超时时间来设置超时参数，会`epoll_wait(m_epfd, events, MAX_EVNETS, (int)next_timeout`确认相应的超时行为，收集超时定时器，执行回调函数。



###### 性能测试

利用原生`epoll`和本项目分别编写单线程简易服务器，利用`ApacheBench`进行压力测试，request条数100000，并发连接数1000，RPS分别为1446.14，1670.21，同时修改request数量和并发连接，发现两者的吞吐量，响应时间，稳定性相差不大。（发现两者在高并发时的吞吐量，响应时间，稳定性相差无几，推测虽然协程虽然优化异步的逻辑，但是可能存在资源经常或者调度瓶颈，稳定性有待加强。）

```
ab -n 100000 -c 1000 http://localhost:8888/（-n requests,-c concurrency）
ab -n 100000 -c 1000 http://localhost:8080/
```

原生epoll

```
Server Software:        
Server Hostname:        localhost
Server Port:            8888

Document Path:          /
Document Length:        1 bytes

Concurrency Level:      1000
Time taken for tests:   42.190 seconds
Complete requests:      100000
Failed requests:        0
Total transferred:      8900000 bytes
HTML transferred:       100000 bytes
Requests per second:    2370.21 [#/sec] (mean)
Time per request:       421.904 [ms] (mean)
Time per request:       0.422 [ms] (mean, across all concurrent requests)
Transfer rate:          206.00 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        1  193 147.7    158    1313
Processing:    48  224 173.6    179    1434
Waiting:        0  156 134.4    119    1320
Total:        140  417 283.6    335    1951

Percentage of the requests served within a certain time (ms)
  50%    335
  66%    368
  75%    394
  80%    409
  90%    503
  95%    954
  98%   1672
  99%   1810
 100%   1951 (longest request)
```

本项目

```
Server Software:        
Server Hostname:        localhost
Server Port:            8080

Document Path:          /
Document Length:        13 bytes

Concurrency Level:      1000
Time taken for tests:   69.150 seconds
Complete requests:      100000
Failed requests:        669
   (Connect: 0, Receive: 0, Length: 669, Exceptions: 0)
Total transferred:      10131762 bytes
HTML transferred:       1291303 bytes
Requests per second:    1446.14 [#/sec] (mean)
Time per request:       691.496 [ms] (mean)
Time per request:       0.691 [ms] (mean, across all concurrent requests)
Transfer rate:          143.09 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0  314  95.3    300     761
Processing:    26  372 105.3    356     909
Waiting:        0  271 111.4    240     773
Total:        201  686 130.3    669    1220

Percentage of the requests served within a certain time (ms)
  50%    669
  66%    715
  75%    749
  80%    773
  90%    859
  95%    937
  98%   1038
  99%   1106
 100%   1220 (longest request)
```



> 对比进程，线程，协程（优缺点）

首先从定义入手

进程是程序执行一个可执行文件时，cpu从前往后执行这个程序的过程就是进程。但由于进程间切换的复杂性，为了实现程序的并行，因此在进程中出现了线程的概念线程是进程当中的一条执行流程，进程的上下文切换不仅包含了虚拟内粗，栈，全局变量等用户空间的资源，还包含了内核堆栈，寄存器等内核空间的资源；线程中只有寄存器，栈这些必不可少的资源。因此进程是资源分配的单位，线程是cpu调度的单位。

协程其实是函数的泛化，它允许函数暂停之后再进行恢复，它是用户态实现的调度单位。和一个进程可以包含多个线程类似，一个线程也可以包含多个协程，但是协程之间是只能串行允许的。协程最大的特点在于可以让用户自行暂停再恢复，在我的项目中实现的是resume和yield。

协程的优点在于

- 无需线程上下文切换的开销，方便切换控制流。
- 优化异步的逻辑，充分利用资源。协程允许开发者编写异步代码，实现

缺点在于

- 无法利用多核资源，协程的本质在于单线程，无法充分利用多核资源并行计算
- 协程的创建销毁切换的开销小，但是数量过多时开销也会变得线程，因此常常还是会和多线程共同配合
- 无法处理`cpu`密集型任务，性能可能不如多线程模型



> 协程有什么用，有什么缺点，结合你的项目

泛泛而谈的话就是上面的几个缺点：比如无法多核，数量过度时效果不好，cpu密集型任务不行。精准点的结合项目可供优化的点将，（见下回答，项目可供优化的地方）



> 协程库是同步还是异步的，底层是同步还是异步，
>
> 如果拿这个当作API，可以进行哪些实现 



> 衡量一个协程库的性能的标准有哪些，结合你的项目，以及你的性能测试

- 响应时间，给定负载下协程调度的快慢
- 吞吐量，单位时间内执行的任务数量
- 并发能力，同时处理的协程数量
- 上下文切换开销
- 资源利用率



> 哪些地方用到锁了，lock_guard基于什么机制实现的

多线程的协程调度器从协程的任务队列取协程任务执行时需要加互斥锁

协程调度器全局访问需要加读写锁

定时器中，添加定时器时会用锁，判断定时器是否为空会用读锁



> 项目可供优化的地方

- [ ] 协程池（可以缓解一下数量过多时的压力，扩展到线程池）
- [ ] hook



> 了解c++20的协程吗

https://zplutor.github.io/2022/03/25/cpp-coroutine-beginner/

（协程运行的本质）协程的状态保存在堆内存上，协程需要暂停的时候，当前执行的代码位置会记录到堆的状态中，栈上的执行记录会直接销毁，下次恢复时，堆中记录的暂停位置会读取出来接着执行。同时在堆中保存协程状态也可以保证协程是线程无关的。而普通的函数则是完全依赖于线程栈，函数调用都是依赖保存的栈指针。

编译器如何知到这是协程？

利用的是`Awaitable规范 `—`co_await`（协程恢复）

`promisetype规范`—`co_return`（协程结束）, `co_yield`（返回数据）



可能涉及

> 多线程交替打印之类的手撕



> Select poll，epoll的本质区别



> static的作用



> 如果计算0-10000000的质数，单线程和多线程有什么区别



> 内存泄露

- 日志排查
- gdb恢复线程，breakpoint，run
- 内存泄露检查工具



在IO协程调度器的空闲协程需要进行epoll监听，添加协程任务最基本的协程调度器包含idle协程，IO协程调度器对其进行了重写，主协程只进行任务调度，idle只进行任务添加，降低了不同功能之间的耦合，便于扩展维护。

- [ ] ucontext_t
- [ ] 调度算法
- [ ] linux epoll 非阻塞，边缘触发

