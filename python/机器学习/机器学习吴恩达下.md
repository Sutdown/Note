# **吴恩达机器学习系列课程(下)**



—Andrew Ng

【[中英字幕]吴恩达机器学习系列课程】https://www.bilibili.com/video/BV164411b7dx?p=58&vd_source=3fb5d6e30320f23cfaa7814e883f9b2f

## 10

##### Debugging a learning algorithm

After having implemented regularized linear regression, when you test your hypothesis on a set of houses, you find that it makes unacceptably large errors in its predictions. Whta should you try next?

- Get more training examples—high variance
- Try smaller sets of features—high variance
- Try getting additional features—high bias
- Try adding polynominal features—high bias
- Try decreasing $\lambda$​—high bias 
- Try increasing $\lambda$—high variance
- …



#### Machine learning diagnostic:

Diagnostic: A test that you can run to gain insight what is/isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance.

Diagnostics can take time to implement, but doing so can be a very use of your time.



#### Advice for applying machine learning

##### Evaluating a hypothesis

Training/Testing procedure for linear regression

- Learn parameter $\theta$ from training data(minimizing train error $J(\theta)$)
- Compute test set error
- Misclassification error(0/1 misclassification error)



##### Model selection and training/validation/test sets

Train/validation/test error



##### Diagnosing bias（underfit) vs. variance(overfit)

cross validation error(测试误差/交叉验证误差)

training error(训练误差)



##### Regularication and bias/variance

linear regression with regularization

正则化参数$\lambda $— choose the right regularization

再利用cross validation error and training error的图像判断bias or variance



##### Learning curves

$J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$​训练误差：是不是一个容易学习

$J_{cv}(\theta)=\frac{1}{2m}\sum_{i=1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^2$​​测试误差：学习方法对位置的测试数据集的预测能力

###### high bias

Sum1: if a learning algorithm is suffering from high bias, getting more training data will not help much.

###### high variance

Sum2: if a learning algorithm is suffering from high variance, getting more training data is likely to help. 



##### Deciding what to try next (revisited)



## 11

#### Machine learning system design

##### Prioritizing what to work on: Spam classification example

###### How to spend your time to make it have low error?

- Collect lots of data
- Develop sophisticated features  based on email routing information(from email header)
- Develop sophisticated features for message body.
- Develop sophisticated algorithm to detect misspellings.

#####  

##### Error analysis

###### Recommended approach

- Start with a simple algorithm that you can implement quickly.Implement it and test it on your cross-validation data.
- Plot learning curves to decide if more data,more features,etc. are likely to help.
- Error analysis: Manually examine the examples (in cross validation set) that your algorithm made errors on.See if you spot any systematic trend in what type of examples it is making errors on.



###### The importance of numerical evaluation 

Error analysis may not be helpful for deciding if this is likely to improve performance.Only solution is to try it and see if it works.

Need numerical evaluation (eg. cross validation error) of algorithm’s performance with and without stemming.



##### Error metrics for skewed classes

###### Precision/Recall

y=1 in presence of rare class that we want to detect 

- Precision查准率

  $precision=\frac{\text{true positives}}{\text{no. of predicted positive}}$

- Recall召回率

  $recall=\frac{\text{true positives}}{\text{no.of actual positive}}$

  

##### Trading off precision and recall

More generally: predict 1 if $h_{\theta}(x)\geq threshold$



how to compare precision/recall numbers?

Answer: $F_1$ Score $2\frac{PR}{P+R}$



#####  Data for machine learning

large data rationale

- Use a learning algorithm with many parameters
- Use a very large training set



## 12

#### Support Vector Machines

##### Optimization objective

 Alternative view of logistic regression

$h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}$

 —> cost function

##### Support vector machine:

$min_\theta C[\sum_{i=1}^mcost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$



##### Large Margin intuition

SVM Decision Boundary:

if $y^{(i)}$=1, we want $\theta^Tx^{(i)}\geq1$(not just $\geq0$)

if $y^{(i)}$=0, we want $\theta^Tx^{(i)}\leq-1$​(not just<0)

- Linearly separable case
- Large margin classifier in presence of outliers 

  

##### The mathematics behind large margin classification(optional)

- vector inner product 

- SVM Decision Boundary 

  $min_\theta\frac{1}{2}\sum_{j=1}^n\theta_j^2=\frac{1}{2}||\theta||^2$

  s.t.  $p^{(i)}\cdot||\theta||\geq1$  if $y^{(i)}=1$

  ​       $p^{(i)}\cdot||\theta||\leq-1$  if $y^{(i)}=1$

  where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\theta $

  Simplification: $\theta_0=0$



#### Kernels

Given x, compute new feature depending on proximity to landmarks $l^{(1)},l^{(2)},l^{(2)}$

$f_1=similarity(x,l^{(i)})=exp(-\frac{||x-l^(1)||^2}{2\sigma^2})$

if $x\approx l^{(1)}:f_1\approx1$

if x is far from $l^{(1)}:f_1\approx0$



Given $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$

choose $l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}$

Given example x:

​     $f_1$=similarity(x,$l^{(1)}$)

​     $f_2$=similarity(x,$l^{(2)}$​)

​     …

For training example ($x^{(i)},y^{(i)}$):

$f_j^{(i)}=sim(x^{(i)},l^{(j)})$

$f_i^{(i)}=sim(x^{(i)},l^{(i)})=1$



##### SVM parameters:  

###### C(=$\frac1\lambda$)

Large C : Lower bias, high variance.(underfit)

Small C : Higher bias, low variance.(overfit)

###### $\sigma^2$

Large $\sigma^2$: Features $f_i$​ vary more smoothly. higher bias,lower variance.

Small $\sigma^2$: Features $f_i$ vary more smoothly. lower bias,higher variance.



##### Using an SVM

Use SVM software package (e.r. liblinear,libsvm,…) to solve for parameters $\theta $

Need to specify:

- Choice of parameter C.

- Choice of kernel(similarity function):

  Eg.No kernel (“linear kernel”) / Gaussian kernel

- Need to choose $\sigma^2$



##### Other choices of kernel 

Note: not all aimilarity functions similarity make valid kernels(Need to satisfy technical condition called “Mercer’s Theorem” to make sure SVM packages’ optimizations run correctly, and do not diverge).



Many off-the-shelf kernels available:

- Polynomial kernel
- More esoteric: String kernel,chi-square kernel, histogram intersection kernel,…



###### Multi-class classfication



###### Logistic regression vs. SVMs

n=number of features , m= number of training examples 

- if n is large (relative to m):

  Use logistic regression, or SVM without a kernel (“linear kernel”)

- if n is small, m is intermediate:

  Use SVM with Gaussian kernel

- if n is small, m is large:

  Create/add more features, then use logistic regression or SVM without a kernel

Neural network likely to work well for most of these setting,but may be slower to train.



## 13

#### Clustering

##### Unsupervised learning introduction

- Applications of clustering



##### K-means algorithm

Input:

- K(number of clusters)

- Training set {$x^{(1)},x^{(2)},...,x^{(m)}$}

  $x^{(i)}\in \mathbb{R}^n$ (drop $x_0=1$​ convention)

k-means for non-separated clusters



##### Optimization objective

###### k-means optimization objective

$c^{(i)}=\text{index of cluster(1,2,...,k) to which example }x^{(i)} \text{ is currently assigned}$

$\mu_k=\text{cluster centroid }k$ $(\mu_k\in\mathbb{R}^n)$

$\mu_{c^{(i)}}=\text{cluster centroid of cluster to which example }x^{(i)} \text{has been assigned}$

Optimization objective:

min $J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_k)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}^2$



##### Random initialization

- Should have k<m
- Randomly pick K training examples
- Set $\mu_1,...,\mu_k$ equal to these K examples.

Pick clustering that gave lowest cost $J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_k)$



local optima: 多次进行不同的初始化 



##### Choosing the number of clusters

- Elbow method: 

  Cost function J（y轴） and K(no. of clusters) （x轴）

- for some later/downstream purpose. 

  Evaluate k-means based on a metric for how well it performs for that later purpose.



## 14

#### Dimensionality Reduction

##### Motivation 1 : Data Compression

##### Motivation 2 : Data Visualization



##### Principal Component Analysisi problem formulation(PCA)

###### Data preprocessing 

Training set : $x^{(1)},x^{(2)},...,x^{(m)}$

preprocessing(feature scaling/mean normalization)

###### PCA algorithm

Reduce data from n-dimensions to k-dimensions

Compute “covariance matrix”:$\Sigma=\frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T$

Compute “eigenvectors” of matrix $\Sigma$: [U,S,V]=svd(Sigma);

–>取U前k列（n*k），z=U转置✖x（n×1）

–>z(k*1)



##### Choosing the number of principal components

###### choosing k (number of principal components)

- Average squared projection error

- Total variation in the data

- Typically ,choose k to be smallest value so that $\frac{\frac1m\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac1m\sum_{i=1}^m||x^{(i)}||^2}\leq0.01$ or $\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^mS_{ii}}\geq0.99$

  “99% of variance is retained”



##### Reconstruction from compressed representation



##### Advice for applying PCA

- Supervised learning speedup 
- Compression
  - Reduce memory/disk needed to store data
  - Speed up learning algorithm
- Visualization
- Bad use of PCA: To prevent overfitting
- PCA is somtimes used where it should not be.

Before implementing PCA ,first try running whatever you want to da with the original/raw data $x^{(i)}.$ Only if that does not do whta you want,then implement PCA and consider using $z^{(i)}$.



## 15

#### Anomaly detection

##### Gaussian/normal distribution

$X\backsim N(\mu,\sigma^2)$



##### Algorithm

###### Density estimation

Training set: {$x^{(1)},...,x^{(m)}$}

Each example is $x\in\mathbb{R}^n$

###### Anomaly dection algorithm

1. choose features $x_i$ that you think might be indicative of anomalous examples.

2. Fit parameters $\mu_1,...,\mu_n$ , $\sigma_1^2,...,\sigma_n^2$

3. Given new eample x,compute p(x):

   $p(x)=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)$

   Anomaly if p(x) < $\varepsilon$

   

##### Developing and evaluating an anomaly detection system

###### The importance of real-number evalution

When developing a learning algorithm, making decisions is much easier if we have a way of evaluating our learning algorithm.



##### Algorithm evaluation

- Fit model p(x) on trainning set {$x^{(1)},…,x^{(m)}$}

- On a cross validation/test example x,predict
- Possible evaluation metrics:
  - True positive,FP,FN,TN
  - Precision/Recall
  - $F_1$-score
- Can alse use cross validation set to choode parameter $\epsilon$



##### Anomaly dection vs. supervised learning

######  Anomaly dection

- Very small number of positive examples.
- Large number of negative examples.



###### Supervised learning

- Large number of positive and negative examples



##### Choosing what features to use

###### non-gaussian features

###### Error analysis for anomaly detection

- Want p(x) large for normal ezamples x

  p(x) small for anomalous examples x.

- More common problem:

  p(x) is comparable(say,both large) for normal and anolalous examples

###### Monitoring computers in a data center

- Choose features that might take on unusually large or small values in the event of an anomaly



##### Multivariate Gaussian distribution

-  $x\in\mathbb{R}^n$.Don’t model $p(x_1),p(x_2),$…,etc.separately.
- Model p(x) all in one go.
- Parameters: $\mu\in \mathbb{R}^n,\Sigma\in\mathbb{R}^{n\times n}$(covariance matrix)



##### Anomaly detection suing the multivariate Gaussian distribution

1. Fit model p(x) by setting

   $\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$​

   $\Sigma =\frac1m\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$

2. Given a new example x , compute

   $p(x)=\frac{1}{(2\pi)^{\frac n2}|\Sigma|^{\frac 12}}$​

   Flag an anomaly if p(x)<$\epsilon $

   

###### Original model

###### Multivariate Gaussian

<img src="C:\Users\向菲\AppData\Roaming\Typora\typora-user-images\image-20240229214158879.png" alt="image-20240229214158879" style="zoom:60%;" />



## 16

#### Recommender Systems

##### Problem formulation

$\theta^{(j)}=\text{parameter vector for user j}$

$x^{(i)}=\text{feature vector for movie i}$

$\text{For user j,movie i,predicted rating: }(\theta^{(j)})^T(x^{(i)})$



##### Content-based recommendations 



##### Collaborative filtering

##### Collaborative filtering algorithm

1. Initialize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$ to small random values.
2. Minimize $J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})$ using gradient descent(or an advanced aptimization algorithm).
3. For a user with parameters $\theta $ and a movie with learned features x,predict a star rating of $\theta^Tx$.



##### Vectirization: Low rank matrix factorization

###### Low rank matrix factorization

###### Finding related movies

Find the 5 movies j with the smallest $||x^{(i)}-x^{(j)}||$​



##### Implementational detail: Mean normalization

$min\frac12\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(x,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2$



## 17

#### Large scale machine learning

##### Learning with large datasets

##### batch gradient descent

Use all m examples in each iteration

##### stochastic gradient descent

Use 1 exampe in each iteration

##### Mini-batch gradient descent

Use b exampes in each iteration



##### Online learning

##### Map-reduce and data parallelism



## 18

#### Application example Photo OCR

##### Problem description and pipeline

 Image—>**Text dectection**—>**Character segmentation**—>**Character recognition** 



##### Sliding window detection 

##### Getting lots of data : Artifical data synthesis

###### Synthedizing data by introducing distortions

###### Discussion on getting more data

1. Make sure you have a low bias classifier before expending the effort. 
2.  “How much work it would it be to get 10x as much data as we currently have?”
   - Artificial data synthesis
   - Collect/label it yourself
   - “Crowd source”



##### Ceiling  analysis: What part of the pipeline to work on text

 

## 19

#### Main topics

- Supervised learning
  - linear regression,logistic regression,neural networks,SVMs
- Unsupervised learning
  - K-means,PCA,Anomaly detection
- Special applications/special topics
  - Recommender systems,large scale machine learning
- Advice on building a machine learning system
  - Bias/variance,regularization;deciding whta to work on next ;evaluation of learning slgorithms,learning cuives,error analysis,ceiling analysis